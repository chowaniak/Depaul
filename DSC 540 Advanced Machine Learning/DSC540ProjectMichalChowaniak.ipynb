{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Parameters: \n",
      " cross val= 1 , binning= 1 , bin ct= 2 , feature selected= 1 , fs type= 4 , grid search= 0\n",
      "['target', 'age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal']\n",
      "Header of target variable:  target\n",
      "303 303\n",
      "\n",
      "\n",
      "--FEATURE SELECTION ON-- \n",
      "\n",
      "Wrapper Select - SVC: \n",
      "Features Selected: ['sex', 'cp', 'fbs', 'exang', 'slope', 'ca', 'thal'] \n",
      " Features Removed: ['age', 'trestbps', 'chol', 'restecg', 'thalach', 'oldpeak']\n",
      "Features (total/selected): 13 7\n",
      "\n",
      "\n",
      "--ML Model Output-- \n",
      "\n",
      "Parameters:  {'activation': 'logistic', 'alpha': 0.0001, 'batch_size': 'auto', 'beta_1': 0.9, 'beta_2': 0.999, 'early_stopping': False, 'epsilon': 1e-08, 'hidden_layer_sizes': (25,), 'learning_rate': 'constant', 'learning_rate_init': 0.001, 'max_iter': 200, 'momentum': 0.9, 'n_iter_no_change': 10, 'nesterovs_momentum': True, 'power_t': 0.5, 'random_state': 0, 'shuffle': True, 'solver': 'adam', 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': False, 'warm_start': False}\n",
      "Neural Network, cross validation, Acc: 0.81 (+/- 0.10)\n",
      "Neural Network, cross validation, AUC: 0.87 (+/- 0.08)\n",
      "Neural Network, cross validation, Recall: 0.77 (+/- 0.17)\n",
      "Neural Network, cross validation, Precision: 0.83 (+/- 0.22)\n",
      "Neural Network, cross validation, Runtime: 1.6940548419952393 \n",
      "\n",
      "\n",
      " Confusion matrix whole dataset\n",
      "\n",
      "\n",
      "            Disease  No disease\n",
      "Disease         137          27\n",
      "No disease       29         110\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 324x324 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "############################################################################################################ \n",
    "#\n",
    "#                                      DSC 540 PROJECT\n",
    "#                                    HEART DISEASE DATASET\n",
    "#\n",
    "############################################################################################################ \n",
    "\n",
    "#Michal Chowaniak\n",
    "\n",
    "\n",
    "import sys\n",
    "import csv\n",
    "import math\n",
    "import numpy as np\n",
    "from operator import itemgetter\n",
    "import time\n",
    "import copy\n",
    "\n",
    "from sklearn.metrics import make_scorer, accuracy_score, roc_auc_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.datasets import make_classification \n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.feature_selection import RFE, VarianceThreshold, SelectFromModel\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_regression, mutual_info_classif, chi2\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_validate, train_test_split\n",
    "from sklearn.preprocessing import KBinsDiscretizer, scale\n",
    "from tempfile import TemporaryFile\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "#Handle annoying warnings\n",
    "import warnings, sklearn.exceptions\n",
    "warnings.filterwarnings(\"ignore\", category=sklearn.exceptions.ConvergenceWarning)\n",
    "\n",
    "\n",
    "############################################################################################################ \n",
    "#\n",
    "#                                      Global parameters\n",
    "#\n",
    "############################################################################################################ \n",
    "\n",
    "\n",
    "\n",
    "#Set global model parameters\n",
    "target_idx=0                                       #Index of Target variable\n",
    "cross_val=1                                         #Control Switch for CV\n",
    "norm_target=0                                       #Normalize target switch\n",
    "norm_features=0                                     #Normalize target switch\n",
    "binning=1                                           #Control Switch for Bin Target\n",
    "bin_cnt=2                                           #If bin target, this sets number of classes\n",
    "feat_select=1                                       #Control Switch for Feature Selection\n",
    "fs_type=4  #4                                           #Feature Selection type: \n",
    "                                                        # 1=Stepwise Backwards Removal, \n",
    "                                                        # 2=Wrapper Select via model Random Forest\n",
    "                                                        # 3=Wrapper Select via model Gradient Boosting\n",
    "                                                        # 4=Wrapper Select via model SVC\n",
    "                                                        # 5=Univariate Feature Selection - Chi-squared\n",
    "                                                        # 6=Full-blown Wrapper Select (from any kind of ML model)\n",
    "lv_filter=0                                         #Control switch for low variance filter on features\n",
    "feat_start=1                                        #Start column of features\n",
    "k_cnt=7                                             #Number of 'Top k' best ranked features to select, only applies for fs_types 1 and 3\n",
    "gridsearchcv=0    #1                                  #control switch for grid search cv \n",
    "rand_st=0                                           #Set Random State variable for randomizing splits on runs\n",
    "\n",
    "# run only certain models\n",
    "dtcv = 1                                            #decistion tree, cross validation only\n",
    "rfcv = 0                                            #random forest, cross validation only\n",
    "gdcv = 0                                            #gradient boosting, cross validation only   \n",
    "abcv = 0                                            #ada boost, cross validation only\n",
    "nncv = 0                                            #neural network, cross validation only\n",
    "svmcv = 0                                           #svm, cross validation only\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print ('Global Parameters: \\n',\n",
    "        'cross val=',cross_val,\n",
    "        ', binning=',binning, \n",
    "        ', bin ct=', bin_cnt, \n",
    "        ', feature selected=', feat_select,\n",
    "        ', fs type=',fs_type,\n",
    "        ', grid search=',gridsearchcv)\n",
    "\n",
    "\n",
    "############################################################################################################ \n",
    "#\n",
    "#                                        Wrapper Feat Select Helper\n",
    "#\n",
    "############################################################################################################ \n",
    "\n",
    "\n",
    "\n",
    "#Recursive Function for searching thru feature space\n",
    "def feat_space_search(arr, curr_idx):\n",
    "    '''Setup currently as exhuastive search, but could be changed to use\n",
    "       greedy search, random search, genetic algorithms, etc. ... also\n",
    "       no regularization, so probably selects more features than necessary'''\n",
    "    global roll_idx, combo_ctr, best_score, sel_idx\n",
    "    \n",
    "    if curr_idx==feat_cnt:\n",
    "        #If end of feature array, roll thru combinations\n",
    "        roll_idx=roll_idx+1\n",
    "        print (\"Combos Searched so far:\", combo_ctr, \"Current Best Score:\", best_score)\n",
    "        for i in range(roll_idx, len(arr)):\n",
    "            arr[i]=0\n",
    "        if roll_idx<feat_cnt-1:\n",
    "            feat_space_search(arr, roll_idx+1)                                                                      #Recurse till end of rolls\n",
    "        \n",
    "    else:\n",
    "        #Else setup next feature combination and calc performance\n",
    "        arr[curr_idx]=1\n",
    "        data=data_np#_wrap                                                                                          #Temp array to hold data\n",
    "        temp_del=[i for i in range(len(arr)) if arr[i]==0]                                                          #Pick out features not in this combo, and remove\n",
    "        data = np.delete(data, temp_del, axis=1)\n",
    "        data_train, data_test, target_train, target_test = train_test_split(data, target_np, test_size=0.35)                \n",
    "\n",
    "        if binning==1:\n",
    "            if bin_cnt<=2:\n",
    "                scorers = {'Accuracy': 'accuracy', 'roc_auc': 'roc_auc'}\n",
    "                scores = cross_validate(clf, data_np, target_np, scoring=scorers, cv=5) \n",
    "                score = scores['test_roc_auc'].mean()                                                               #AUC\n",
    "            else:\n",
    "                sscorers = {'Accuracy': 'accuracy'}\n",
    "                scores = cross_validate(clf, data_np, target_np, scoring=scorers, cv=5) \n",
    "                score = scores['test_Accuracy'].mean()                                                              #Accuracy\n",
    "            print('Acc/AUC:', curr_idx, feat_arr, len(data[0]), score)\n",
    "            if score>best_score:                                                                                    #Compare performance and update sel_idx and best_score, if needed\n",
    "                best_score=score\n",
    "                sel_idx=copy.deepcopy(arr) \n",
    "                \n",
    "        if binning==0:\n",
    "            scorers = {'Neg_MSE': 'neg_mean_squared_error', 'expl_var': 'explained_variance'}\n",
    "            scores = cross_validate(rgr, data, target_np, scoring=scorers, cv=5)    \n",
    "            score = np.asarray([math.sqrt(-x) for x in scores['test_Neg_MSE']]).mean()                              #RMSE\n",
    "            print('RMSE:', curr_idx, feat_arr, len(data[0]), score)\n",
    "            if score<best_score:                                                                                    #Compare performance and update sel_idx and best_score, if needed\n",
    "                best_score=score\n",
    "                sel_idx=copy.deepcopy(arr) \n",
    "\n",
    "        #move to next feature index and recurse\n",
    "        combo_ctr+=1  \n",
    "        curr_idx+=1\n",
    "        feat_space_search(arr, curr_idx)                                                                            #Recurse till end of iteration for roll\n",
    "\n",
    "\n",
    "\n",
    "############################################################################################################ \n",
    "#\n",
    "#                                        Load Data\n",
    "#\n",
    "############################################################################################################ \n",
    "\n",
    "file1= csv.reader(open('DataSet/processedcleveland2.csv'), delimiter=',', quotechar='\"')\n",
    "#file1= csv.reader(open('Feature Selection/processedclevelandfeatures.csv'), delimiter=',', quotechar='\"')\n",
    "\n",
    "#Read Header Line\n",
    "header=next(file1)\n",
    "\n",
    "#Read data\n",
    "data=[]\n",
    "target=[]\n",
    "for row in file1:\n",
    "    #Load Target\n",
    "    if row[target_idx]=='':                         #If target is blank, skip row                       \n",
    "        continue\n",
    "    else:\n",
    "        target.append(float(row[target_idx]))       #If pre-binned class, change float to int\n",
    "\n",
    "    #Load row into temp array, cast columns  \n",
    "    temp=[]\n",
    "                 \n",
    "    for j in range(feat_start,len(header)):\n",
    "        if row[j]=='':\n",
    "            temp.append(float())\n",
    "        else:\n",
    "            try:\n",
    "                temp.append(float(row[j]))               \n",
    "            except ValueError:\n",
    "                temp.append(9999)\n",
    "                                \n",
    "    #Load temp into Data array\n",
    "    data.append(temp)\n",
    "    \n",
    "    \n",
    "print(header)\n",
    "print (\"Header of target variable: \", header[target_idx])\n",
    "print(len(target),len(data))\n",
    "#for i in range(10):\n",
    "#    print(target[i])\n",
    "#    print(data[i])\n",
    "print('\\n')\n",
    "\n",
    "\n",
    "data_np=np.asarray(data)\n",
    "\n",
    "#replace missign values with median value\n",
    "imp = SimpleImputer(missing_values=9999, strategy='median')\n",
    "imp.fit(data_np)\n",
    "data_np=imp.transform(data_np)\n",
    "\n",
    "\n",
    "#save data to a file\n",
    "#np.savetxt(\"test.csv\", data_np, delimiter=\",\")\n",
    "#np.savetxt(\"testcorrected.csv\", imp.transform(data_np), delimiter=\",\")\n",
    "\n",
    "target_np=np.asarray(target)\n",
    "\n",
    "#print (target_np)\n",
    "\n",
    "\n",
    "############################################################################################################ \n",
    "#\n",
    "#                                        Preprocess data\n",
    "#\n",
    "############################################################################################################ \n",
    "\n",
    "\n",
    "#recode values 2,3,4 as 1\n",
    "#0 = no disease, 1=disease present\n",
    "\n",
    "target_np[target_np > 1] = 1\n",
    "#print (target_np)\n",
    "\n",
    "#target histogram\n",
    "plt.hist(target_np)\n",
    "plt.title(\"Histogram\")\n",
    "plt.xlabel(\"Value\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "#plt.xticks([0,1])\n",
    "\n",
    "\n",
    "#barplot(target_np)\n",
    "#hist(taget_np)\n",
    "\n",
    "\n",
    "\n",
    "if norm_target==1:\n",
    "    #Target normalization for continuous values\n",
    "    target_np=scale(target_np)\n",
    "\n",
    "if norm_features==1:\n",
    "    #Feature normalization for continuous values\n",
    "    data_np=scale(data_np)\n",
    "\n",
    "'''if binning==1:\n",
    "    \n",
    "    #Discretize Target variable with KBinsDiscretizer\n",
    "    enc = KBinsDiscretizer(n_bins=[bin_cnt], encode='ordinal', strategy='quantile')         #Strategy here is important, quantile creating equal bins, but kmeans prob being more valid \"clusters\"\n",
    "    target_np_bin = enc.fit_transform(target_np.reshape(-1,1))\n",
    "\n",
    "    #Get Bin min/max\n",
    "    temp=[[] for x in range(bin_cnt+1)]\n",
    "    for i in range(len(target_np)):\n",
    "        for j in range(bin_cnt):\n",
    "            if target_np_bin[i]==j:\n",
    "                temp[j].append(target_np[i])\n",
    "\n",
    "    print (temp, \"*****test\" )\n",
    "    for j in range(bin_cnt):\n",
    "        print('Bin', j, ':', min(temp[j]), max(temp[j]), len(temp[j]))\n",
    "    print('\\n')\n",
    "\n",
    "    #Convert Target array back to correct shape\n",
    "    target_np=np.ravel(target_np_bin)'''\n",
    "\n",
    "\n",
    "############################################################################################################ \n",
    "#\n",
    "#                                        Feature Selection\n",
    "#\n",
    "############################################################################################################ \n",
    "\n",
    "\n",
    "#Low Variance Filter\n",
    "if lv_filter==1:\n",
    "    print('--LOW VARIANCE FILTER ON--', '\\n')\n",
    "    \n",
    "    #LV Threshold\n",
    "    sel = VarianceThreshold(threshold=0.5)                                      #Removes any feature with less than 20% variance\n",
    "    fit_mod=sel.fit(data_np)\n",
    "    fitted=sel.transform(data_np)\n",
    "    sel_idx=fit_mod.get_support()\n",
    "\n",
    "    #Get lists of selected and non-selected features (names and indexes)\n",
    "    temp=[]\n",
    "    temp_idx=[]\n",
    "    temp_del=[]\n",
    "    for i in range(len(data_np[0])):\n",
    "        if sel_idx[i]==1:                                                           #Selected Features get added to temp header\n",
    "            temp.append(header[i+feat_start])\n",
    "            temp_idx.append(i)\n",
    "        else:                                                                       #Indexes of non-selected features get added to delete array\n",
    "            temp_del.append(i)\n",
    "\n",
    "    print('Selected', temp)\n",
    "    print('Features (total, selected):', len(data_np[0]), len(temp))\n",
    "    print('\\n')\n",
    "\n",
    "    #Filter selected columns from original dataset\n",
    "    header = header[0:feat_start]\n",
    "    for field in temp:\n",
    "        header.append(field)\n",
    "    data_np = np.delete(data_np, temp_del, axis=1)                                 #Deletes non-selected features by index\n",
    "\n",
    "\n",
    "#Feature Selection\n",
    "if feat_select==1:\n",
    "    '''Three steps:\n",
    "       1) Run Feature Selection\n",
    "       2) Get lists of selected and non-selected features\n",
    "       3) Filter columns from original dataset\n",
    "       '''\n",
    "    \n",
    "    print('--FEATURE SELECTION ON--', '\\n')\n",
    "    \n",
    "    ##1) Run Feature Selection #######\n",
    "    if fs_type==1:\n",
    "        #Stepwise Recursive Backwards Feature removal\n",
    "        if binning==1:\n",
    "            clf = RandomForestClassifier(n_estimators=200, max_depth=None, min_samples_split=3, criterion='entropy', random_state=None)\n",
    "            sel = RFE(clf, n_features_to_select=k_cnt, step=.1)\n",
    "            print('Stepwise Recursive Backwards - Random Forest: ')\n",
    "        if binning==0:\n",
    "            rgr = RandomForestRegressor(n_estimators=500, max_depth=None, min_samples_split=3, criterion='mse', random_state=None)\n",
    "            sel = RFE(rgr, n_features_to_select=k_cnt, step=.1)\n",
    "            print('Stepwise Recursive Backwards - Random Forest: ')\n",
    "            \n",
    "        fit_mod=sel.fit(data_np, target_np)\n",
    "        print(sel.ranking_)\n",
    "        sel_idx=fit_mod.get_support()      \n",
    "\n",
    "    if fs_type==2:\n",
    "        #Wrapper Select via model Random Forest\n",
    "        if binning==1:\n",
    "            clf = RandomForestClassifier(n_estimators=200, max_depth=None, min_samples_split=3, criterion='entropy', random_state=None)\n",
    "            sel = SelectFromModel(clf, prefit=False, threshold='mean', max_features=None)                                                           #to select only based on max_features, set to integer value and set threshold=-np.inf\n",
    "            print ('Wrapper Select - Random Forest: ')\n",
    "        if binning==0:\n",
    "            rgr = RandomForestRegressor(n_estimators=500, max_features=.33, max_depth=None, min_samples_split=3, criterion='mse', random_state=None)\n",
    "            sel = SelectFromModel(rgr, prefit=False, threshold='mean', max_features=None)\n",
    "            print ('Wrapper Select - Random Forest: ')\n",
    "            \n",
    "        fit_mod=sel.fit(data_np, target_np)\n",
    "        #print(sel.ranking_)\n",
    "        sel_idx=fit_mod.get_support()\n",
    "    \n",
    "    \n",
    "    if fs_type==3:\n",
    "        #Wrapper Select via model Gradient Boosting\n",
    "        if binning==1:\n",
    "            clf = GradientBoostingClassifier(n_estimators=100, loss= 'deviance', learning_rate=0.1, max_depth=3, min_samples_split=3, random_state=rand_st)    \n",
    "            sel = SelectFromModel(clf, prefit=False, threshold='mean', max_features=None)                                                           #to select only based on max_features, set to integer value and set threshold=-np.inf\n",
    "            print ('Wrapper Select - Gradient Boosting: ')\n",
    "        if binning==0:\n",
    "            rgr = GradientBoostingRegressor(n_estimators=100, loss='ls', learning_rate=0.1, max_depth=3, min_samples_split=3, random_state=rand_st)        \n",
    "            sel = SelectFromModel(rgr, prefit=False, threshold='mean', max_features=None)\n",
    "            print ('Wrapper Select - Gradient Boosting: ')\n",
    "            \n",
    "        fit_mod=sel.fit(data_np, target_np)\n",
    "        sel_idx=fit_mod.get_support()\n",
    "    \n",
    "        \n",
    "    if fs_type==4:\n",
    "        #Wrapper Select via model SVC\n",
    "        if binning==1:\n",
    "            clf = SVC(kernel='linear', gamma= 'scale', C=1.0, probability=True, random_state=rand_st) #'''Replace comment HERE'''\n",
    "            sel = SelectFromModel(clf, prefit=False, threshold='mean', max_features=None)                                                           #to select only based on max_features, set to integer value and set threshold=-np.inf\n",
    "            print ('Wrapper Select - SVC: ')\n",
    "        if binning==0:\n",
    "            rgr = '''Unused in this homework'''\n",
    "            sel = SelectFromModel(rgr, prefit=False, threshold='mean', max_features=None)\n",
    "            print ('Wrapper Select - SVC: ')\n",
    "            \n",
    "        fit_mod=sel.fit(data_np, target_np) \n",
    "        sel_idx=fit_mod.get_support()\n",
    "    \n",
    "\n",
    "    if fs_type==5:       \n",
    "        if binning==1:                                                              ######Only work if the Target is binned###########\n",
    "            #Univariate Feature Selection - Chi-squared\n",
    "            sel=SelectKBest(chi2, k=k_cnt)\n",
    "            fit_mod=sel.fit(data_np, target_np)                                         #will throw error if any negative values in features, so turn off feature normalization, or switch to mutual_info_classif\n",
    "            print ('Univariate Feature Selection - Chi2: ')\n",
    "            sel_idx=fit_mod.get_support()\n",
    "\n",
    "        if binning==0:                                                              ######Only work if the Target is continuous###########\n",
    "            #Univariate Feature Selection - Mutual Info Regression\n",
    "            sel=SelectKBest(mutual_info_regression, k=k_cnt)\n",
    "            fit_mod=sel.fit(data_np, target_np)\n",
    "            print ('Univariate Feature Selection - Mutual Info: ')\n",
    "            sel_idx=fit_mod.get_support()\n",
    "\n",
    "        #Print ranked variables out sorted\n",
    "        temp=[]\n",
    "        scores=fit_mod.scores_\n",
    "        for i in range(feat_start, len(header)):            \n",
    "            temp.append([header[i], float(scores[i-feat_start])])\n",
    "\n",
    "        print('Ranked Features')\n",
    "        temp_sort=sorted(temp, key=itemgetter(1), reverse=True)\n",
    "        for i in range(len(temp_sort)):\n",
    "            print(i, temp_sort[i][0], ':', temp_sort[i][1])\n",
    "        print('\\n')\n",
    "\n",
    "    if fs_type==6:\n",
    "        #Full-blown Wrapper Select (from any kind of ML model)        \n",
    "        if binning==1:                                                              ######Only work if the Target is binned###########\n",
    "            start_ts=time.time()\n",
    "            sel_idx=[]                                                                                      #Empty array to hold optimal selected feature set\n",
    "            best_score=0                                                                                    #For classification compare Accuracy or AUC, higher is better, so start with 0\n",
    "            feat_cnt=len(data_np[0])\n",
    "            #Create Wrapper model\n",
    "            clf = SVC(kernel='linear', gamma= 'scale', C=1.0, probability=True, random_state=rand_st)                                #This could be any kind of classifier model\n",
    "      \n",
    "        if binning==0:                                                              ######Only work if the Target is continuous###########\n",
    "            start_ts=time.time()\n",
    "            sel_idx=[]                                                                                      #Empty array to hold optimal selected feature set\n",
    "            best_score=sys.float_info.max                                                                   #For regression compare RMSE, lower is better, so start with max sys float value\n",
    "            feat_cnt=len(data_np[0])\n",
    "            #Create Wrapper model\n",
    "            rgr = SVR(kernel='linear', gamma=0.1, C=1.0)#'''Replace comment HERE'''                    #This could be any kind of regressor model         \n",
    "        \n",
    "        #Loop thru feature sets\n",
    "        roll_idx=0\n",
    "        combo_ctr=0\n",
    "        feat_arr=[0 for col in range(feat_cnt)]                                         #Initialize feature array\n",
    "        for idx in range(feat_cnt):\n",
    "            roll_idx=idx\n",
    "            feat_space_search(feat_arr, idx)                                           #Recurse\n",
    "            feat_arr=[0 for col in range(feat_cnt)]                                     #Reset feature array after each iteration\n",
    "        \n",
    "        print('# of Feature Combos Tested:', combo_ctr)\n",
    "        print(best_score, sel_idx, len(data_np[0]))\n",
    "        print(\"Wrapper Feat Sel Runtime:\", time.time()-start_ts)\n",
    "             \n",
    "        \n",
    "     ##2) Get lists of selected and non-selected features (names and indexes) #######\n",
    "    temp, temp4 = ([] for i in range(2))\n",
    "    temp_idx=[]\n",
    "    temp_del=[]\n",
    "    for i in range(len(data_np[0])):\n",
    "        if sel_idx[i]==1:                                                           #Selected Features get added to temp header\n",
    "            temp.append(header[i+feat_start])\n",
    "            temp_idx.append(i)\n",
    "        else: \n",
    "            temp4.append(header[i+feat_start]) #Indexes of non-selected features get added to delete array\n",
    "            temp_del.append(i)\n",
    "    print('Features Selected:', temp, \"\\n\" ,'Features Removed:', temp4)\n",
    "    print('Features (total/selected):', len(data_np[0]), len(temp))\n",
    "    print('\\n')\n",
    "            \n",
    "                \n",
    "    ##3) Filter selected columns from original dataset #########\n",
    "    header = header[0:feat_start]\n",
    "    for field in temp:\n",
    "        header.append(field)\n",
    "    data_np = np.delete(data_np, temp_del, axis=1)                                 #Deletes non-selected features by index)\n",
    "    \n",
    "\n",
    "\n",
    "############################################################################################################    \n",
    "#\n",
    "#                                          Train SciKit Models\n",
    "#\n",
    "#                                           Test/Train split\n",
    "#\n",
    "############################################################################################################\n",
    "\n",
    "\n",
    "print('--ML Model Output--', '\\n')\n",
    "\n",
    "#Test/Train split\n",
    "data_train, data_test, target_train, target_test = train_test_split(data_np, target_np, test_size=0.35)\n",
    "\n",
    "####Classifiers####\n",
    "if binning==1 and cross_val==0:\n",
    "    #SciKit Decision Tree\n",
    "    start_ts=time.time()\n",
    "    clf = DecisionTreeClassifier(criterion='entropy', splitter='best', max_depth=None, min_samples_split=3, min_samples_leaf=1, max_features=None, random_state=rand_st)\n",
    "    clf.fit(data_train, target_train)\n",
    "    param = clf.get_params()\n",
    "    print ('Parameters: ', param)\n",
    "    print('Decision Tree , test/tain, Acc: %0.2f'  %clf.score(data_test, target_test))\n",
    "    if bin_cnt<=2:                                                                                                  #AUC only works with binary classes, not multiclass\n",
    "        print('Decision Tree, test/tain, AUC: %0.2f' % metrics.roc_auc_score(target_test, clf.predict_proba(data_test)[:,1]))             \n",
    "        #joblib.dump(clf, 'DecTree_DSC540_HW1.pkl')                     #Save and pickle model\n",
    "    print(\"Decision Tree, test/tain, Runtime: \", time.time()-start_ts, \"\\n\")\n",
    "   \n",
    "    \n",
    "    \n",
    "    #SciKit Random Forest\n",
    "    start_ts=time.time()\n",
    "    clf = RandomForestClassifier(n_estimators=200, max_depth=None, min_samples_split=3, criterion='entropy', random_state=None)\n",
    "    clf.fit(data_train, target_train)\n",
    "    param = clf.get_params()\n",
    "    print ('Parameters: ', param)\n",
    "    print('Random Forest, test/tain,  Acc: %0.2f' % clf.score(data_test, target_test))\n",
    "    if bin_cnt<=2:                                                                                                  #AUC only works with binary classes, not multiclass\n",
    "        print('Random Forest , test/tain, AUC: %0.2f' % metrics.roc_auc_score(target_test, clf.predict_proba(data_test)[:,1])) \n",
    "    print(\"Random Forest , test/tain, Runtime:\", time.time()-start_ts, \"\\n\")\n",
    "        \n",
    "        \n",
    "\n",
    "    #SciKit Gradient Boosting test/train split\n",
    "    start_ts=time.time()\n",
    "    clf = GradientBoostingClassifier(n_estimators=100, loss= 'deviance', learning_rate=0.1, max_depth=md, min_samples_split=3, random_state=rand_st)\n",
    "    clf.fit(data_train, target_train)\n",
    "    param = clf.get_params()\n",
    "    print ('Parameters: ', param)\n",
    "    print('Gradient Boosting, test/tain,  Acc: %0.2f' % clf.score(data_test, target_test))\n",
    "    if bin_cnt<=2:                                                                                                  #AUC only works with binary classes, not multiclass\n",
    "        print('Gradient Boosting , test/tain, AUC: %0.2f' % metrics.roc_auc_score(target_test, clf.predict_proba(data_test)[:,1]))\n",
    "    print(\"Gradient Boosting , test/tain, Runtime:\", time.time()-start_ts, \"\\n\")\n",
    "        \n",
    "    #SciKit ADA Boost test/train split\n",
    "    start_ts=time.time()\n",
    "    clf = AdaBoostClassifier(n_estimators=100, base_estimator = None, learning_rate=0.1, random_state=rand_st)\n",
    "    clf.fit(data_train, target_train)\n",
    "    param = clf.get_params()\n",
    "    print ('Parameters: ', param)\n",
    "    print('ADA Boost, test/tain,  Acc: %0.2f' % clf.score(data_test, target_test))\n",
    "    if bin_cnt<=2:                                                                                                  #AUC only works with binary classes, not multiclass\n",
    "        print('ADA Boost , test/tain, AUC: %0.2f' % metrics.roc_auc_score(target_test, clf.predict_proba(data_test)[:,1]))\n",
    "    print(\"ADA Boost, test/tain,Runtime:\", time.time()-start_ts, \"\\n\")\n",
    "    \n",
    "    #SciKit Neral Network test/train split\n",
    "    start_ts=time.time()\n",
    "    clf = MLPClassifier(activation= 'logistic', solver= 'adam', alpha=0.0001, max_iter=1000, hidden_layer_sizes=(10, ), random_state=rand_st )\n",
    "    clf.fit(data_train, target_train)\n",
    "    param = clf.get_params()\n",
    "    print ('Parameters: ', param)\n",
    "    print('Neural Network, test/tain,  Acc: %0.2f' % clf.score(data_test, target_test))\n",
    "    if bin_cnt<=2:                                                                                                  #AUC only works with binary classes, not multiclass\n",
    "        print('Neural Network, test/tain,  AUC: %0.2f' % metrics.roc_auc_score(target_test, clf.predict_proba(data_test)[:,1]))\n",
    "    print(\"Neural Network, test/tain, Runtime:\", time.time()-start_ts, \"\\n\")\n",
    "\n",
    "    #SciKit CVS test/train split\n",
    "    start_ts=time.time()\n",
    "    clf = SVC(kernel='linear', gamma= 'scale', C=1.0, probability=True, random_state=rand_st)\n",
    "    clf.fit(data_train, target_train)\n",
    "    param = clf.get_params()\n",
    "    print ('Parameters: ', param)\n",
    "    print('SVC , test/tain,  Acc: %0.2f' % clf.score(data_test, target_test))\n",
    "    if bin_cnt<=2:                                                                                                  #AUC only works with binary classes, not multiclass\n",
    "        print('SVC, test/tain, AUC: %0.2f' % metrics.roc_auc_score(target_test, clf.predict_proba(data_test)[:,1]))\n",
    "    print(\"SVC , test/tain, Runtime:\", time.time()-start_ts, \"\\n\")\n",
    "\n",
    "    \n",
    "############################################################################################################    \n",
    "#\n",
    "#                                         Train and Test SciKit Models\n",
    "#\n",
    "#                                 Binning and Cross-Valalidation Classifiers\n",
    "#\n",
    "############################################################################################################\n",
    "\n",
    "\n",
    "if binning==1 and cross_val==1 and gridsearchcv ==0: \n",
    "    \n",
    "    \n",
    "    #Setup Crossval classifier scorers\n",
    "    if bin_cnt<=2:\n",
    "        #scorers = {'Accuracy': 'accuracy', 'roc_auc': 'roc_auc'}\n",
    "        scorers = {'Accuracy': 'accuracy', 'roc_auc': 'roc_auc', 'Recall': 'recall', 'Precision': 'precision' }\n",
    "    else:\n",
    "        scorers = {'Accuracy': 'accuracy'}\n",
    "\n",
    "     \n",
    "    \n",
    "    \n",
    "    if dtcv ==1:\n",
    "        #SciKit Decision Tree - Cross Val\n",
    "        start_ts=time.time()\n",
    "        clf = DecisionTreeClassifier(criterion='entropy', splitter='best', max_depth=None, min_samples_split=3, min_samples_leaf=1, max_features=None, random_state=rand_st)\n",
    "        param = clf.get_params()\n",
    "        print ('Parameters: ', param)\n",
    "        scores = cross_validate(clf, data_np, target_np, scoring=scorers, cv=5)\n",
    "        scores_Acc = scores['test_Accuracy']\n",
    "        print(\"Decision Tree, cross validation, Acc: %0.2f (+/- %0.2f)\" % (scores_Acc.mean(), scores_Acc.std() * 2))        \n",
    "        if bin_cnt<=2:                                                                                                  #Only works with binary classes, not multiclass\n",
    "            scores_AUC= scores['test_roc_auc']\n",
    "            scores_Recall = scores['test_Recall']\n",
    "            scores_Precis = scores['test_Precision']\n",
    "            print(\"Decision Tree, cross validation, AUC: %0.2f (+/- %0.2f)\" % (scores_AUC.mean(), scores_AUC.std() * 2))                           \n",
    "            print(\"Decision Tree, cross validation, Recall: %0.2f (+/- %0.2f)\" % (scores_Recall.mean(), scores_Recall.std() * 2))\n",
    "            print(\"Decision Tree, cross validation, Precision: %0.2f (+/- %0.2f)\" % (scores_Precis.mean(), scores_Precis.std() * 2))\n",
    "        print(\"Decision Tree, cross validation, Runtime:\", time.time()-start_ts, \"\\n\")\n",
    "        #print(scores.keys())\n",
    "      \n",
    "    \n",
    "#         #confusion matrix test data\n",
    "#         print('\\n')\n",
    "#         clf_fit = clf.fit(data_train, target_train)\n",
    "#         predictions = clf_fit.predict(data_test)\n",
    "#         cm = confusion_matrix(target_test, predictions)\n",
    "#         print(\"Confusion Matrix on test set 35%: \\n\", cm)\n",
    "#         print('\\n')\n",
    "#         #Plot confusion matrix\n",
    "#         df = pd.DataFrame(cm, index = ['heart disease', 'no heart disease'], columns = ['heart disease', 'no heart disease'])\n",
    "#         plt.figure(figsize=(4,4))\n",
    "#         sns.heatmap(df, annot=True)\n",
    "#         plt.title('Confusion Matrix')\n",
    "#         plt.ylabel('True label')\n",
    "#         plt.xlabel('Predicted label')\n",
    "#         plt.show()\n",
    "#         print('\\n')\n",
    "    \n",
    " \n",
    "\n",
    "        #confusion matrix all data\n",
    "        print('\\n', 'Confusion matrix whole dataset')\n",
    "        clf_fit = clf.fit(data_np, target_np)\n",
    "        predictions = clf_fit.predict(data_np)\n",
    "        cm = confusion_matrix(target_np, predictions)\n",
    "        #print(\"Confusion Matrix on test set 35%: \\n\", cm)\n",
    "        print('\\n')\n",
    "        #Plot confusion matrix\n",
    "        df = pd.DataFrame(cm, index = ['Disease', 'No disease'], columns = ['Disease', 'No disease'])\n",
    "        print(df)\n",
    "        plt.figure(figsize=(4.5,4.5))\n",
    "        sns.heatmap(df, annot=True, fmt='.0f')\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.ylabel('True label')\n",
    "        plt.xlabel('Predicted label')\n",
    "        plt.show()\n",
    "        print('\\n')\n",
    "\n",
    "\n",
    "#         #confusion matrix all data\n",
    "#         print('\\n', 'confusion matrix all data')\n",
    "#         clf_fit = clf.fit(data_np, target_np)\n",
    "#         predictions = clf_fit.predict(data_np)\n",
    "#         cm = confusion_matrix(target_np, predictions)\n",
    "#         print(\"Confusion Matrix on test set 35%: \\n\", cm)\n",
    "#         print('\\n')\n",
    "#         #Plot confusion matrix\n",
    "#         df = pd.DataFrame(cm, index = ['Disease', 'No disease'], columns = ['Disease', 'No disease'])\n",
    "#         print (df)\n",
    "        \n",
    "#         s= df.sum().sum()\n",
    "#         print (s)\n",
    "#         dfperc = df/s\n",
    "#         print (dfperc)\n",
    "        \n",
    "#         plt.figure(figsize=(4.5,4.5))\n",
    "#         sns.heatmap(dfperc, annot=True, fmt='.2%')\n",
    "#         #for t in ax.texts: t.set_text(t.get_text() + \" %\")\n",
    "               \n",
    "#         plt.title('Confusion Matrix %')\n",
    "#         plt.ylabel('True label')\n",
    "#         plt.xlabel('Predicted label')\n",
    "#         plt.show()\n",
    "#         print('\\n')\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    if rfcv ==1:\n",
    "        #SciKit Random Forest - Cross Val\n",
    "        start_ts=time.time()\n",
    "        #clf = RandomForestClassifier(n_estimators=200, max_depth=None, min_samples_split=3, criterion='entropy', random_state=None)\n",
    "        #clf = RandomForestClassifier(n_estimators=100, max_depth=None, min_samples_split=3,criterion='entropy', random_state=None)\n",
    "        #clf = RandomForestClassifier(n_estimators=300, max_depth=None, min_samples_split=3,criterion='entropy', random_state=None)\n",
    "        #clf = RandomForestClassifier(n_estimators=500, max_depth=None, min_samples_split=3,criterion='entropy', random_state=None)\n",
    "        #clf = RandomForestClassifier(n_estimators=800, max_depth=None, min_samples_split=3,criterion='entropy', random_state=None)\n",
    "        #clf = RandomForestClassifier(n_estimators=1000, max_depth=None, min_samples_split=3,criterion='entropy', random_state=None)\n",
    "        #clf = RandomForestClassifier(n_estimators=1500, max_depth=None, min_samples_split=3,criterion='entropy', random_state=None)\n",
    "        #clf = RandomForestClassifier(n_estimators=2000, max_depth=None, min_samples_split=3,criterion='entropy', random_state=None)\n",
    "        \n",
    "        #clf = RandomForestClassifier(n_estimators=200, max_depth=None, min_samples_split=3, criterion='gini', random_state=None)\n",
    "        #clf = RandomForestClassifier(n_estimators=100, max_depth=None, min_samples_split=3,criterion='gini', random_state=None)\n",
    "        #clf = RandomForestClassifier(n_estimators=300, max_depth=None, min_samples_split=3,criterion='gini', random_state=None)\n",
    "        #clf = RandomForestClassifier(n_estimators=500, max_depth=None, min_samples_split=3,criterion='gini', random_state=None)\n",
    "        #clf = RandomForestClassifier(n_estimators=800, max_depth=None, min_samples_split=3,criterion='gini', random_state=None)\n",
    "        #clf = RandomForestClassifier(n_estimators=1000, max_depth=None, min_samples_split=3,criterion='gini', random_state=None)\n",
    "        #clf = RandomForestClassifier(n_estimators=1500, max_depth=None, min_samples_split=3,criterion='gini', random_state=None)\n",
    "        #clf = RandomForestClassifier(n_estimators=2000, max_depth=None, min_samples_split=3,criterion='gini', random_state=None)\n",
    "        \n",
    "        #clf = RandomForestClassifier(n_estimators=1000, max_depth=None, min_samples_split=2,criterion='entropy', random_state=None)\n",
    "        #clf = RandomForestClassifier(n_estimators=1000, max_depth=None, min_samples_split=5,criterion='entropy', random_state=None)\n",
    "        #clf = RandomForestClassifier(n_estimators=1000, max_depth=None, min_samples_split=8,criterion='entropy', random_state=None)\n",
    "        #clf = RandomForestClassifier(n_estimators=1000, max_depth=None, min_samples_split=10,criterion='entropy', random_state=None)    \n",
    "        #clf = RandomForestClassifier(n_estimators=1000, max_depth=None, min_samples_split=15,criterion='entropy', random_state=None)    \n",
    "        #clf = RandomForestClassifier(n_estimators=1000, max_depth=None, min_samples_split=20,criterion='entropy', random_state=None)    \n",
    "        clf = RandomForestClassifier(n_estimators=1000, max_depth=None, min_samples_split=30,criterion='entropy', random_state=None)    \n",
    "        #clf = RandomForestClassifier(n_estimators=1000, max_depth=None, min_samples_split=40,criterion='entropy', random_state=None)    \n",
    "        \n",
    "        param = clf.get_params()\n",
    "        print ('Parameters: ', param)\n",
    "        scores = cross_validate(clf, data_np, target_np, scoring=scorers, cv=5)\n",
    "        scores_Acc = scores['test_Accuracy']\n",
    "        print(\"Random Forest, cross validation, Acc: %0.2f (+/- %0.2f)\" % (scores_Acc.mean(), scores_Acc.std() * 2))        \n",
    "        if bin_cnt<=2:                                                                                                  #Only works with binary classes, not multiclass\n",
    "            scores_AUC= scores['test_roc_auc']\n",
    "            scores_Recall = scores['test_Recall']\n",
    "            scores_Precis = scores['test_Precision']\n",
    "            print(\"Random Forest, cross validation, AUC: %0.2f (+/- %0.2f)\" % (scores_AUC.mean(), scores_AUC.std() * 2))\n",
    "            print(\"Random Forest, cross validation, Recall: %0.2f (+/- %0.2f)\" % (scores_Recall.mean(), scores_Recall.std() * 2))\n",
    "            print(\"Random Forest, cross validation, Precision: %0.2f (+/- %0.2f)\" % (scores_Precis.mean(), scores_Precis.std() * 2))\n",
    "        print(\"Random Forest, cross validation, Runtime:\", time.time()-start_ts, \"\\n\")\n",
    "\n",
    "#         #confusion matrix\n",
    "#         print('\\n')\n",
    "#         clf_fit = clf.fit(data_train, target_train)\n",
    "#         predictions = clf_fit.predict(data_test)\n",
    "#         cm = confusion_matrix(target_test, predictions)\n",
    "#         print(\"Confusion Matrix on test set 35%: \\n\", cm)\n",
    "#         print('\\n')\n",
    "#         #Plot confusion matrix\n",
    "#         df = pd.DataFrame(cm, index = ['heart disease', 'no heart disease'], columns = ['heart disease', 'no heart disease'])\n",
    "#         plt.figure(figsize=(4,4))\n",
    "#         sns.heatmap(df, annot=True)\n",
    "#         plt.title('Confusion Matrix')\n",
    "#         plt.ylabel('True label')\n",
    "#         plt.xlabel('Predicted label')\n",
    "#         plt.show()\n",
    "#         print('\\n')\n",
    "        \n",
    "        #confusion matrix all data\n",
    "        print('\\n', 'Confusion matrix whole dataset')\n",
    "        clf_fit = clf.fit(data_np, target_np)\n",
    "        predictions = clf_fit.predict(data_np)\n",
    "        cm = confusion_matrix(target_np, predictions)\n",
    "        #print(\"Confusion Matrix on test set 35%: \\n\", cm)\n",
    "        print('\\n')\n",
    "        #Plot confusion matrix\n",
    "        df = pd.DataFrame(cm, index = ['Disease', 'No disease'], columns = ['Disease', 'No disease'])\n",
    "        print(df)\n",
    "        plt.figure(figsize=(4.5,4.5))\n",
    "        sns.heatmap(df, annot=True, fmt='.0f')\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.ylabel('True label')\n",
    "        plt.xlabel('Predicted label')\n",
    "        plt.show()\n",
    "        print('\\n')\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    if gdcv == 1:\n",
    "        #SciKit Gradient Boosting - Cross Val\n",
    "        #Number of trees, learning rate/step size, loss function deviance’, ‘exponential’, \n",
    "        #max depth of each tree \n",
    "        start_ts=time.time()\n",
    "        #clf=GradientBoostingClassifier(n_estimators=100, loss= 'deviance', learning_rate=0.1, max_depth=3, min_samples_split=3, random_state=rand_st) #'''Replace comment HERE'''\n",
    "        #clf=GradientBoostingClassifier(n_estimators=200, loss= 'deviance', learning_rate=0.1, max_depth=3, min_samples_split=3, random_state=rand_st)\n",
    "        #clf=GradientBoostingClassifier(n_estimators=300, loss= 'deviance', learning_rate=0.1, max_depth=3, min_samples_split=3, random_state=rand_st)\n",
    "        #clf=GradientBoostingClassifier(n_estimators=500, loss= 'deviance', learning_rate=0.1, max_depth=3, min_samples_split=3, random_state=rand_st)\n",
    "        #clf=GradientBoostingClassifier(n_estimators=50, loss= 'deviance', learning_rate=0.1, max_depth=3, min_samples_split=3, random_state=rand_st)\n",
    "        #clf=GradientBoostingClassifier(n_estimators=25, loss= 'deviance', learning_rate=0.1, max_depth=3, min_samples_split=3, random_state=rand_st)\n",
    "        #clf=GradientBoostingClassifier(n_estimators=10, loss= 'deviance', learning_rate=0.1, max_depth=3, min_samples_split=3, random_state=rand_st)\n",
    "        \n",
    "        #clf=GradientBoostingClassifier(n_estimators=100, loss= 'exponential', learning_rate=0.1, max_depth=3, min_samples_split=3, random_state=rand_st) #'''Replace comment HERE'''\n",
    "        #clf=GradientBoostingClassifier(n_estimators=200, loss= 'exponential', learning_rate=0.1, max_depth=3, min_samples_split=3, random_state=rand_st)\n",
    "        #clf=GradientBoostingClassifier(n_estimators=300, loss= 'exponential', learning_rate=0.1, max_depth=3, min_samples_split=3, random_state=rand_st)\n",
    "        #clf=GradientBoostingClassifier(n_estimators=500, loss= 'exponential', learning_rate=0.1, max_depth=3, min_samples_split=3, random_state=rand_st)\n",
    "        #clf=GradientBoostingClassifier(n_estimators=50, loss= 'exponential', learning_rate=0.1, max_depth=3, min_samples_split=3, random_state=rand_st)\n",
    "        #clf=GradientBoostingClassifier(n_estimators=25, loss= 'exponential', learning_rate=0.1, max_depth=3, min_samples_split=3, random_state=rand_st)\n",
    "        #clf=GradientBoostingClassifier(n_estimators=10, loss= 'exponential', learning_rate=0.1, max_depth=3, min_samples_split=3, random_state=rand_st)\n",
    "        \n",
    "        #clf=GradientBoostingClassifier(n_estimators=25, loss= 'exponential', learning_rate=0.15, max_depth=3, min_samples_split=3, random_state=rand_st)\n",
    "        #clf=GradientBoostingClassifier(n_estimators=25, loss= 'deviance', learning_rate=0.15, max_depth=3, min_samples_split=3, random_state=rand_st)\n",
    "        #clf=GradientBoostingClassifier(n_estimators=25, loss= 'deviance', learning_rate=0.2, max_depth=3, min_samples_split=3, random_state=rand_st)\n",
    "        #clf=GradientBoostingClassifier(n_estimators=25, loss= 'deviance', learning_rate=0.05, max_depth=3, min_samples_split=3, random_state=rand_st)\n",
    "        \n",
    "        #clf=GradientBoostingClassifier(n_estimators=25, loss= 'exponential', learning_rate=0.1, max_depth=2, min_samples_split=3, random_state=rand_st)\n",
    "        #clf=GradientBoostingClassifier(n_estimators=25, loss= 'deviance', learning_rate=0.1, max_depth=2, min_samples_split=3, random_state=rand_st)\n",
    "        \n",
    "        #clf=GradientBoostingClassifier(n_estimators=25, loss= 'exponential', learning_rate=0.1, max_depth=4, min_samples_split=3, random_state=rand_st)\n",
    "        #clf=GradientBoostingClassifier(n_estimators=25, loss= 'deviance', learning_rate=0.1, max_depth=4, min_samples_split=3, random_state=rand_st)\n",
    "        \n",
    "        clf=GradientBoostingClassifier(n_estimators=25, loss= 'exponential', learning_rate=0.05, max_depth=3, min_samples_split=3, random_state=rand_st)\n",
    "        \n",
    "        param = clf.get_params()\n",
    "        print ('Parameters: ', param)\n",
    "        scores= cross_validate(clf, data_np, target_np, scoring=scorers, cv=5)#'''Replace comment HERE'''\n",
    "        scores_Acc = scores['test_Accuracy']                                                                                                                                                                                                                                 \n",
    "        scores_AUC= scores['test_roc_auc']\n",
    "        scores_Recall = scores['test_Recall']\n",
    "        scores_Precis = scores['test_Precision']\n",
    "        print(\"Gradient Boosting, cross validation, Acc: %0.2f (+/- %0.2f)\" % (scores_Acc.mean(), scores_Acc.std() * 2))       \n",
    "        print(\"Gradient Boosting, cross validation, AUC: %0.2f (+/- %0.2f)\" % (scores_AUC.mean(), scores_AUC.std() * 2))\n",
    "        print(\"Gradient Boosting, cross validation, Recall: %0.2f (+/- %0.2f)\" % (scores_Recall.mean(), scores_Recall.std() * 2))\n",
    "        print(\"Gradient Boosting, cross validation, Precision: %0.2f (+/- %0.2f)\" % (scores_Precis.mean(), scores_Precis.std() * 2))                 \n",
    "        print(\"Gradient Boosting, cross validation, Runtime:\", time.time()-start_ts, \"\\n\")\n",
    "        endtime = time.time()\n",
    "        t = (endtime-start_ts) #running time\n",
    "        gbacc = scores_Acc.mean() # accuracy\n",
    "        gbauc = scores_AUC.mean() # auc\n",
    "    \n",
    "#         #confusion matrix\n",
    "#         print('\\n')\n",
    "#         clf_fit = clf.fit(data_train, target_train)\n",
    "#         predictions = clf_fit.predict(data_test)\n",
    "#         cm = confusion_matrix(target_test, predictions)\n",
    "#         print(\"Confusion Matrix on test set 35%: \\n\", cm)\n",
    "#         print('\\n')\n",
    "#         #Plot confusion matrix\n",
    "#         df = pd.DataFrame(cm, index = ['heart disease', 'no heart disease'], columns = ['heart disease', 'no heart disease'])\n",
    "#         plt.figure(figsize=(4,4))\n",
    "#         sns.heatmap(df, annot=True)\n",
    "#         plt.title('Confusion Matrix')\n",
    "#         plt.ylabel('True label')\n",
    "#         plt.xlabel('Predicted label')\n",
    "#         plt.show()\n",
    "#         print('\\n')\n",
    "    \n",
    "        #confusion matrix all data\n",
    "        print('\\n', 'Confusion matrix whole dataset')\n",
    "        clf_fit = clf.fit(data_np, target_np)\n",
    "        predictions = clf_fit.predict(data_np)\n",
    "        cm = confusion_matrix(target_np, predictions)\n",
    "        #print(\"Confusion Matrix on test set 35%: \\n\", cm)\n",
    "        print('\\n')\n",
    "        #Plot confusion matrix\n",
    "        df = pd.DataFrame(cm, index = ['Disease', 'No disease'], columns = ['Disease', 'No disease'])\n",
    "        print(df)\n",
    "        plt.figure(figsize=(4.5,4.5))\n",
    "        sns.heatmap(df, annot=True, fmt='.0f')\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.ylabel('True label')\n",
    "        plt.xlabel('Predicted label')\n",
    "        plt.show()\n",
    "        print('\\n')\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    if abcv ==1:\n",
    "        #SciKit Ada Boosting - Cross Val\n",
    "        start_ts=time.time()\n",
    "        #clf=AdaBoostClassifier(n_estimators=20, base_estimator = None, learning_rate=0.1, random_state=rand_st) #'''Replace comment HERE'''\n",
    "        #clf=AdaBoostClassifier(n_estimators=50, base_estimator = None, learning_rate=0.1, random_state=rand_st)\n",
    "        clf=AdaBoostClassifier(n_estimators=100, base_estimator = None, learning_rate=0.1, random_state=rand_st)\n",
    "        #clf=AdaBoostClassifier(n_estimators=150, base_estimator = None, learning_rate=0.1, random_state=rand_st)\n",
    "        #clf=AdaBoostClassifier(n_estimators=200, base_estimator = None, learning_rate=0.1, random_state=rand_st)\n",
    "        #clf=AdaBoostClassifier(n_estimators=300, base_estimator = None, learning_rate=0.1, random_state=rand_st)\n",
    "        #clf=AdaBoostClassifier(n_estimators=500, base_estimator = None, learning_rate=0.1, random_state=rand_st)\n",
    "        \n",
    "        #clf=AdaBoostClassifier(n_estimators=100, base_estimator = None, learning_rate=0.05, random_state=rand_st)\n",
    "        #clf=AdaBoostClassifier(n_estimators=100, base_estimator = None, learning_rate=0.15, random_state=rand_st)\n",
    "        #clf=AdaBoostClassifier(n_estimators=100, base_estimator = None, learning_rate=0.2, random_state=rand_st)\n",
    "        \n",
    "        #base = RandomForestClassifier(n_estimators=1000, max_depth=None, min_samples_split=30,criterion='entropy', random_state=None)\n",
    "        #clf=AdaBoostClassifier(n_estimators=100, base_estimator = base, learning_rate=0.1, random_state=rand_st)    \n",
    "        \n",
    "        #base = RandomForestClassifier(n_estimators=1000, max_depth=None, min_samples_split=2,criterion='entropy', random_state=None)\n",
    "        #clf=AdaBoostClassifier(n_estimators=100, base_estimator = base, learning_rate=0.1, random_state=rand_st)    \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        param = clf.get_params()\n",
    "        print ('Parameters: ', param)\n",
    "        scores= cross_validate(clf, data_np, target_np, scoring=scorers, cv=5)#'''Replace comment HERE'''\n",
    "        scores_Acc = scores['test_Accuracy']                                                                                                                                                                                                                                 \n",
    "        scores_AUC = scores['test_roc_auc']\n",
    "        scores_Recall = scores['test_Recall']\n",
    "        scores_Precis = scores['test_Precision']\n",
    "        print(\"Ada Boost, cross validation, Acc: %0.2f (+/- %0.2f)\" % (scores_Acc.mean(), scores_Acc.std() * 2))       \n",
    "        print(\"Ada Boost, cross validation, AUC: %0.2f (+/- %0.2f)\" % (scores_AUC.mean(), scores_AUC.std() * 2))\n",
    "        print(\"Ada Boost, cross validation, Recall: %0.2f (+/- %0.2f)\" % (scores_Recall.mean(), scores_Recall.std() * 2))\n",
    "        print(\"Ada Boost, cross validation, Precision: %0.2f (+/- %0.2f)\" % (scores_Precis.mean(), scores_Precis.std() * 2))                 \n",
    "        \n",
    "        \n",
    "        print(\"Ada Boost, cross validation, Runtime:\", time.time()-start_ts, \"\\n\")#'''Replace comment HERE''' \n",
    "\n",
    "#         #confusion matrix\n",
    "#         print('\\n')\n",
    "#         clf_fit = clf.fit(data_train, target_train)\n",
    "#         predictions = clf_fit.predict(data_test)\n",
    "#         cm = confusion_matrix(target_test, predictions)\n",
    "#         print(\"Confusion Matrix on test set 35%: \\n\", cm)\n",
    "#         print('\\n')\n",
    "#         #Plot confusion matrix\n",
    "#         df = pd.DataFrame(cm, index = ['heart disease', 'no heart disease'], columns = ['heart disease', 'no heart disease'])\n",
    "#         plt.figure(figsize=(4,4))\n",
    "#         sns.heatmap(df, annot=True)\n",
    "#         plt.title('Confusion Matrix')\n",
    "#         plt.ylabel('True label')\n",
    "#         plt.xlabel('Predicted label')\n",
    "#         plt.show()\n",
    "#         print('\\n')\n",
    "        \n",
    "        \n",
    "        \n",
    "        #confusion matrix all data\n",
    "        print('\\n', 'Confusion matrix whole dataset')\n",
    "        clf_fit = clf.fit(data_np, target_np)\n",
    "        predictions = clf_fit.predict(data_np)\n",
    "        cm = confusion_matrix(target_np, predictions)\n",
    "        #print(\"Confusion Matrix on test set 35%: \\n\", cm)\n",
    "        print('\\n')\n",
    "        #Plot confusion matrix\n",
    "        df = pd.DataFrame(cm, index = ['Disease', 'No disease'], columns = ['Disease', 'No disease'])\n",
    "        print(df)\n",
    "        plt.figure(figsize=(4.5,4.5))\n",
    "        sns.heatmap(df, annot=True, fmt='.0f')\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.ylabel('True label')\n",
    "        plt.xlabel('Predicted label')\n",
    "        plt.show()\n",
    "        print('\\n')\n",
    "        \n",
    "        \n",
    "        \n",
    "    if nncv == 1:\n",
    "        #SciKit Neural Network - Cross Val\n",
    "        start_ts=time.time()\n",
    "        #clf=MLPClassifier(activation= 'logistic', solver= 'lbfgs', alpha=0.0001, max_iter=1000, hidden_layer_sizes=(10, ), random_state=rand_st )\n",
    "        \n",
    "        #clf=MLPClassifier(activation= 'logistic', solver= 'adam', alpha=0.0001, max_iter=1000, hidden_layer_sizes=(10, ), random_state=rand_st )\n",
    "        #clf=MLPClassifier(activation= 'logistic', solver= 'adam', alpha=0.0001, max_iter=1000, hidden_layer_sizes=(100, ), random_state=rand_st )    \n",
    "        #clf=MLPClassifier(activation= 'logistic', solver= 'adam', alpha=0.0001, max_iter=1000, hidden_layer_sizes=(100,100), random_state=rand_st )    \n",
    "        #clf=MLPClassifier(activation= 'logistic', solver= 'adam', alpha=0.0001, max_iter=1000, hidden_layer_sizes=(100,100,100), random_state=rand_st )    \n",
    "        #clf=MLPClassifier(activation= 'logistic', solver= 'adam', alpha=0.0001, max_iter=1000, hidden_layer_sizes=(50,), random_state=rand_st )    \n",
    "        #clf=MLPClassifier(activation= 'logistic', solver= 'adam', alpha=0.0001, max_iter=1000, hidden_layer_sizes=(50,50), random_state=rand_st )    \n",
    "        #clf=MLPClassifier(activation= 'logistic', solver= 'adam', alpha=0.0001, max_iter=1000, hidden_layer_sizes=(50,50,50), random_state=rand_st )    \n",
    "        #clf=MLPClassifier(activation= 'logistic', solver= 'adam', alpha=0.0001, max_iter=1000, hidden_layer_sizes=(150,), random_state=rand_st )    \n",
    "        #clf=MLPClassifier(activation= 'logistic', solver= 'adam', alpha=0.0001, max_iter=1000, hidden_layer_sizes=(150,150), random_state=rand_st )    \n",
    "        #clf=MLPClassifier(activation= 'logistic', solver= 'adam', alpha=0.0001, max_iter=1000, hidden_layer_sizes=(200,200,200), random_state=rand_st )    \n",
    "        \n",
    "        #clf=MLPClassifier(activation= 'logistic', solver= 'lbfgs', alpha=0.0001, max_iter=1000, hidden_layer_sizes=(10, ), random_state=rand_st )\n",
    "        #clf=MLPClassifier(activation= 'logistic', solver= 'lbfgs', alpha=0.0001, max_iter=1000, hidden_layer_sizes=(10,10 ), random_state=rand_st )\n",
    "        #clf=MLPClassifier(activation= 'logistic', solver= 'lbfgs', alpha=0.0001, max_iter=1000, hidden_layer_sizes=(100, ), random_state=rand_st )    \n",
    "        #clf=MLPClassifier(activation= 'logistic', solver= 'lbfgs', alpha=0.0001, max_iter=1000, hidden_layer_sizes=(100,100), random_state=rand_st )    \n",
    "        #clf=MLPClassifier(activation= 'logistic', solver= 'lbfgs', alpha=0.0001, max_iter=1000, hidden_layer_sizes=(100,100,100), random_state=rand_st )    \n",
    "        \n",
    "        #clf=MLPClassifier(activation= 'identity', solver= 'adam', alpha=0.0001, max_iter=1000, hidden_layer_sizes=(100,100), random_state=rand_st )\n",
    "        #clf=MLPClassifier(activation= 'tanh', solver= 'adam', alpha=0.0001, max_iter=1000, hidden_layer_sizes=(100,100), random_state=rand_st )\n",
    "        #clf=MLPClassifier(activation= 'relu', solver= 'adam', alpha=0.0001, max_iter=1000, hidden_layer_sizes=(100,100), random_state=rand_st )\n",
    "        \n",
    "        #clf=MLPClassifier(activation= 'identity', solver= 'adam', alpha=0.0001, max_iter=1000, hidden_layer_sizes=(100,100), random_state=rand_st )\n",
    "        \n",
    "        clf=MLPClassifier(activation= 'logistic', solver= 'adam',alpha=0.0001, hidden_layer_sizes=(25,), random_state=rand_st )\n",
    "        \n",
    "        #, max_iter=1000\n",
    "        \n",
    "        param = clf.get_params()\n",
    "        print ('Parameters: ', param)\n",
    "        scores= cross_validate(clf, data_np, target_np, scoring=scorers, cv=5)#'''Replace comment HERE'''\n",
    "        scores_Acc = scores['test_Accuracy']                                                                                                                                                                                                                                 \n",
    "        scores_AUC = scores['test_roc_auc']\n",
    "        scores_Recall = scores['test_Recall']\n",
    "        scores_Precis = scores['test_Precision']\n",
    "        print(\"Neural Network, cross validation, Acc: %0.2f (+/- %0.2f)\" % (scores_Acc.mean(), scores_Acc.std() * 2))\n",
    "        print(\"Neural Network, cross validation, AUC: %0.2f (+/- %0.2f)\" % (scores_AUC.mean(), scores_AUC.std() * 2))\n",
    "        print(\"Neural Network, cross validation, Recall: %0.2f (+/- %0.2f)\" % (scores_Recall.mean(), scores_Recall.std() * 2))\n",
    "        print(\"Neural Network, cross validation, Precision: %0.2f (+/- %0.2f)\" % (scores_Precis.mean(), scores_Precis.std() * 2))\n",
    "        print(\"Neural Network, cross validation, Runtime:\", time.time()-start_ts, \"\\n\")# '''Replace comment HERE'''\n",
    "\n",
    "        \n",
    "#         #confusion matrix\n",
    "#         print('\\n')\n",
    "#         clf_fit = clf.fit(data_train, target_train)\n",
    "#         predictions = clf_fit.predict(data_test)\n",
    "#         cm = confusion_matrix(target_test, predictions)\n",
    "#         print(\"Confusion Matrix on test set 35%: \\n\", cm)\n",
    "#         print('\\n')\n",
    "#         #Plot confusion matrix\n",
    "#         df = pd.DataFrame(cm, index = ['heart disease', 'no heart disease'], columns = ['heart disease', 'no heart disease'])\n",
    "#         plt.figure(figsize=(4,4))\n",
    "#         sns.heatmap(df, annot=True)\n",
    "#         plt.title('Confusion Matrix')\n",
    "#         plt.ylabel('True label')\n",
    "#         plt.xlabel('Predicted label')\n",
    "#         plt.show()\n",
    "#         print('\\n')\n",
    "        \n",
    "        #confusion matrix all data\n",
    "        print('\\n', 'Confusion matrix whole dataset')\n",
    "        clf_fit = clf.fit(data_np, target_np)\n",
    "        predictions = clf_fit.predict(data_np)\n",
    "        cm = confusion_matrix(target_np, predictions)\n",
    "        #print(\"Confusion Matrix on test set 35%: \\n\", cm)\n",
    "        print('\\n')\n",
    "        #Plot confusion matrix\n",
    "        df = pd.DataFrame(cm, index = ['Disease', 'No disease'], columns = ['Disease', 'No disease'])\n",
    "        print(df)\n",
    "        plt.figure(figsize=(4.5,4.5))\n",
    "        sns.heatmap(df, annot=True, fmt='.0f')\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.ylabel('True label')\n",
    "        plt.xlabel('Predicted label')\n",
    "        plt.show()\n",
    "        print('\\n')\n",
    "           \n",
    "    if svmcv == 1:\n",
    "        #SciKit SVM - Cross Val\n",
    "        start_ts=time.time()\n",
    "        #clf= SVC(kernel='linear', gamma= 'scale', C=1.0, probability=True, random_state=rand_st) #'''Replace comment HERE'''\n",
    "        #clf= SVC(kernel='linear', gamma= 0.1, C=1.0, probability=True, random_state=rand_st)\n",
    "        #clf= SVC(kernel='linear', gamma= 0.01, C=1.0, probability=True, random_state=rand_st)\n",
    "        #clf= SVC(kernel='linear', gamma= 'auto', C=1.0, probability=True, random_state=rand_st)\n",
    "        #clf= SVC(kernel='poly', gamma= 'scale', C=1.0, probability=True, random_state=rand_st)\n",
    "        #clf= SVC(kernel='poly', gamma= 0.1, C=1.0, probability=True, random_state=rand_st)\n",
    "        #clf= SVC(kernel='poly', gamma= 0.01, C=1.0, probability=True, random_state=rand_st)\n",
    "        #clf= SVC(kernel='poly', gamma= 'auto', C=1.0, probability=True, random_state=rand_st)\n",
    "        #clf= SVC(kernel='sigmoid', gamma= 'scale', C=1.0, probability=True, random_state=rand_st)\n",
    "        #clf= SVC(kernel='sigmoid', gamma= 0.1, C=1.0, probability=True, random_state=rand_st)\n",
    "        #clf= SVC(kernel='sigmoid', gamma= 0.01, C=1.0, probability=True, random_state=rand_st)\n",
    "        #clf= SVC(kernel='sigmoid', gamma= 'auto', C=1.0, probability=True, random_state=rand_st)\n",
    "        #clf= SVC(kernel='rbf', gamma= 'scale', C=1.0, probability=True, random_state=rand_st)\n",
    "        #clf= SVC(kernel='rbf', gamma= 'auto', C=1.0, probability=True, random_state=rand_st)\n",
    "        \n",
    "        clf= SVC(kernel='linear', gamma= 'auto', C=1.0, probability=True, random_state=rand_st)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        param = clf.get_params()\n",
    "        print ('Parameters: ', param)\n",
    "        scores= cross_validate(clf, data_np, target_np, scoring=scorers, cv=5)#'''Replace comment HERE'''\n",
    "        scores_Acc = scores['test_Accuracy']                                                                                                                                                                                                                                 \n",
    "        scores_AUC = scores['test_roc_auc']\n",
    "        scores_Recall = scores['test_Recall']\n",
    "        scores_Precis = scores['test_Precision']\n",
    "        print(\"SVM, cross validation, Acc: %0.2f (+/- %0.2f)\" % (scores_Acc.mean(), scores_Acc.std() * 2))\n",
    "        print(\"SVM, cross validation, AUC: %0.2f (+/- %0.2f)\" % (scores_AUC.mean(), scores_AUC.std() * 2))\n",
    "        print(\"SVM, cross validation, Recall: %0.2f (+/- %0.2f)\" % (scores_Recall.mean(), scores_Recall.std() * 2))\n",
    "        print(\"SVM, cross validation, Precision: %0.2f (+/- %0.2f)\" % (scores_Precis.mean(), scores_Precis.std() * 2))\n",
    "        print(\"SVM, cross validation, Runtime:\", time.time()-start_ts)\n",
    "    \n",
    "        \n",
    "#         #confusion matrix\n",
    "#         print('\\n')\n",
    "#         clf_fit = clf.fit(data_train, target_train)\n",
    "#         predictions = clf_fit.predict(data_test)\n",
    "#         cm = confusion_matrix(target_test, predictions)\n",
    "#         print(\"Confusion Matrix on test set 35%: \\n\", cm)\n",
    "#         print('\\n')\n",
    "#         #Plot confusion matrix\n",
    "#         df = pd.DataFrame(cm, index = ['heart disease', 'no heart disease'], columns = ['heart disease', 'no heart disease'])\n",
    "#         plt.figure(figsize=(4,4))\n",
    "#         sns.heatmap(df, annot=True)\n",
    "#         plt.title('Confusion Matrix')\n",
    "#         plt.ylabel('True label')\n",
    "#         plt.xlabel('Predicted label')\n",
    "#         plt.show()\n",
    "#         print('\\n')\n",
    "        \n",
    "        \n",
    "        #confusion matrix all data\n",
    "        print('\\n', 'Confusion matrix whole dataset')\n",
    "        clf_fit = clf.fit(data_np, target_np)\n",
    "        predictions = clf_fit.predict(data_np)\n",
    "        cm = confusion_matrix(target_np, predictions)\n",
    "        #print(\"Confusion Matrix on test set 35%: \\n\", cm)\n",
    "        print('\\n')\n",
    "        #Plot confusion matrix\n",
    "        df = pd.DataFrame(cm, index = ['Disease', 'No disease'], columns = ['Disease', 'No disease'])\n",
    "        print(df)\n",
    "        plt.figure(figsize=(4.5,4.5))\n",
    "        sns.heatmap(df, annot=True, fmt='.0f')\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.ylabel('True label')\n",
    "        plt.xlabel('Predicted label')\n",
    "        plt.show()\n",
    "        print('\\n')\n",
    "        \n",
    "        \n",
    "\n",
    "############################################################################################################    \n",
    "#\n",
    "#                                       Train and Test SciKit Models\n",
    "#\n",
    "#                          Binning  Cross-Val Classifiers     GridSearchCV\n",
    "#\n",
    "############################################################################################################        \n",
    "        \n",
    "        \n",
    "if binning==1 and cross_val==1 and gridsearchcv ==1:\n",
    "    \n",
    "       \n",
    "    \n",
    "        #Random Forest - gridseach\n",
    "        start_ts=time.time()\n",
    "        \n",
    "        if rfcv ==1: \n",
    "            clf = RandomForestClassifier(random_state=rand_st)\n",
    "            parameters = {'n_estimators': [900,950,1000,1050],\n",
    "                          'max_features': ['log2', 'sqrt','auto', None],\n",
    "                          'criterion': ['entropy'],\n",
    "                          'max_depth': [None, 2, 3, 5, 10],\n",
    "                          'min_samples_split': [28,29,30,31,32,33],\n",
    "                          'min_samples_leaf': [1,5,8]}\n",
    "\n",
    "        #Gradient Boosting - grid search\n",
    "        if gdcv == 1:\n",
    "            clf=GradientBoostingClassifier(random_state=rand_st)\n",
    "            parameters = {'learning_rate': [0.05, 0.1, 0.15, 0.2],\n",
    "                          'loss': ['deviance', 'exponential'],\n",
    "                          'criterion': ['friedman_mse', 'mse', 'mae'],\n",
    "                          'max_depth': [2, 3, 5],\n",
    "                          'n_estimators':[25, 50, 100, 150, 200]\n",
    "                         }\n",
    "            \n",
    "         # ADA Boost - grid search\n",
    "        if abcv == 1:\n",
    "            \n",
    "            base1 = RandomForestClassifier(n_estimators=100, max_depth=None, min_samples_split=3,criterion='gini', random_state=None)    \n",
    "            base2 = RandomForestClassifier(n_estimators=200, max_depth=None, min_samples_split=3,criterion='entropy', random_state=None)        \n",
    "            \n",
    "            clf=AdaBoostClassifier(random_state=rand_st)\n",
    "            \n",
    "            parameters = {'n_estimators':[90,94,100,105,110],\n",
    "                         'learning_rate': [0.08, 0.09, 0.1, 0.11, 0.12],\n",
    "                         'base_estimator': [base1, base2]}\n",
    "           \n",
    "        #Neural Network - grid search\n",
    "        if nncv == 1:                                            \n",
    "\n",
    "            \n",
    "            clf=MLPClassifier(max_iter=1000, random_state=rand_st )\n",
    "            \n",
    "            parameters = {'hidden_layer_sizes': [(100,), (10,10), (10,), (100,100),(12,), (12,12), (20,), (20,20), (30,30) ],\n",
    "                        'activation': ['tanh', 'relu','logistic'],\n",
    "                        'solver': ['sgd', 'adam','lbfgs'],\n",
    "                        'alpha': [0.0001, 0.05],\n",
    "                        'learning_rate': ['constant','adaptive'],\n",
    "                        }\n",
    "            \n",
    "             #SVC - grid search\n",
    "        if svmcv == 1: \n",
    " \n",
    "            parameters ={'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "                         'C': [1,2,3,4,5,10,100],\n",
    "                         'gamma': ['auto', 'scale', 0.1, 0.001, 0.0001],\n",
    "                         'max_iter': [1,-1, 100,1000,10000]\n",
    "                         \n",
    "                        }\n",
    "            \n",
    "       \n",
    "    \n",
    "    \n",
    "        \n",
    "        #scoring\n",
    "        acc_scorer = make_scorer(accuracy_score)\n",
    "\n",
    "        # grid\n",
    "        grid_obj = GridSearchCV(clf, parameters, scoring=acc_scorer, cv=5, iid=True, refit = True, n_jobs=-1 )\n",
    "\n",
    "        grid_obj = grid_obj.fit(data_train, target_train)\n",
    "        results = grid_obj.cv_results_\n",
    "        best_est = grid_obj.best_estimator_\n",
    "        best_sc = grid_obj.best_score_\n",
    "\n",
    "        #fit best estimator\n",
    "        best_est_obj = best_est.fit(data_train, target_train)\n",
    "\n",
    "        #predict\n",
    "        predictions = best_est_obj.predict(data_test)\n",
    "\n",
    "        acc = accuracy_score(target_test, predictions)\n",
    "        print(\"Accuracy: %0.2f \" % acc)\n",
    "        auc_score = metrics.roc_auc_score(target_test, best_est_obj.predict_proba(data_test)[:,1])\n",
    "        print(\"AUC: %0.2f\" % auc_score)\n",
    "        print(\"CV Runtime:\", time.time()-start_ts)\n",
    "        print(best_est)\n",
    "        cm = confusion_matrix(target_test, predictions)\n",
    "        print(\"Confusion Matrix on test set 35%: \\n\", cm)\n",
    "        \n",
    "        \n",
    "        print('\\n')\n",
    "        #Plot confusion matrix\n",
    "        df = pd.DataFrame(cm, index = ['heart disease', 'no heart disease'], columns = ['heart disease', 'no heart disease'])\n",
    "        plt.figure(figsize=(4,4))\n",
    "        sns.heatmap(df, annot=True)\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.ylabel('True label')\n",
    "        plt.xlabel('Predicted label')\n",
    "        plt.show()\n",
    "        print('\\n')\n",
    "         \n",
    "        \n",
    "        \n",
    "        \n",
    "        if bin_cnt<=2:\n",
    "            scorers = {'Accuracy': 'accuracy', 'roc_auc': 'roc_auc', 'Recall': 'recall', 'Precision': 'precision' }\n",
    "            #scorers = {'Accuracy': 'accuracy', 'roc_auc': 'roc_auc'}\n",
    "        else:\n",
    "            scorers = {'Accuracy': 'accuracy'}\n",
    "\n",
    "        start_ts=time.time()    \n",
    "        clf=best_est\n",
    "        param = clf.get_params()\n",
    "        print ('Parameters: ', param)\n",
    "        scores = cross_validate(clf, data_np, target_np, scoring=scorers, cv=5)\n",
    "        scores_Acc = scores['test_Accuracy']\n",
    "        print(\"GridSearch cross validation Acc: %0.2f (+/- %0.2f)\" % (scores_Acc.mean(), scores_Acc.std() * 2))    \n",
    "        if bin_cnt<=2:                                                                                                  #Only works with binary classes, not multiclass\n",
    "            scores_AUC= scores['test_roc_auc']\n",
    "            scores_Recall = scores['test_Recall']\n",
    "            scores_Precis = scores['test_Precision']\n",
    "            print(\"GridSearch cross cross validation, AUC: %0.2f (+/- %0.2f)\" % (scores_AUC.mean(), scores_AUC.std() * 2))\n",
    "            print(\"GridSearch cross cross validation, Recall: %0.2f (+/- %0.2f)\" % (scores_Recall.mean(), scores_Recall.std() * 2))\n",
    "            print(\"GridSearch cross cross validation, Precision: %0.2f (+/- %0.2f)\" % (scores_Precis.mean(), scores_Precis.std() * 2))\n",
    "        print(\"GridSearch cross cross validation, Runtime:\", time.time()-start_ts, \"\\n\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
